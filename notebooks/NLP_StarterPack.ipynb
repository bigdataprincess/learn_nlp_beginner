{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing for Beginners"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "What do I need:\n",
    "    - Understand Python\n",
    "    - Have Anaconda installed or setup a python virtual env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*What is NLP?*\n",
    "- This is a type of data analysis to help computer interprete natural language as a human would.\n",
    "- Process text from documenets usually unstructured text.\n",
    "- is a subset of artificial intelligence that helps computers understand, interpret and manipulate human language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK: Natural Language Tool kit\n",
    "This is one of the most powerful platform/library for analyzing natural languages using Python programming language. \n",
    "\n",
    "Learn More:\n",
    "https://www.nltk.org/"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Starter Pack Overview:\n",
    "    - Counting words\n",
    "    - Frequency Distribution\n",
    "    - N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  We will be downloading text from the free ebooks library gutenberg\n",
    "nltk.download('gutenberg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab words in the shakespeare-hamlet\n",
    "sh_ham = nltk.corpus.gutenberg.words(\"shakespeare-hamlet.txt\")\n",
    "\n",
    "# Preview the first 20 words\n",
    "sh_ham[:20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using .count we can get number of times a word appears\n",
    "sh_ham.count(\"Hamlet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package inaugural to\n",
      "[nltk_data]     /Users/princessiria/nltk_data...\n",
      "[nltk_data]   Package inaugural is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets take a look using inuagural speech\n",
    "nltk.download('inaugural')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.corpus.inaugural.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1789-Washington.txt has a total of 1538 words.\n",
      "1793-Washington.txt has a total of 147 words.\n",
      "1797-Adams.txt has a total of 2585 words.\n",
      "1801-Jefferson.txt has a total of 1935 words.\n",
      "1805-Jefferson.txt has a total of 2384 words.\n",
      "1809-Madison.txt has a total of 1265 words.\n",
      "1813-Madison.txt has a total of 1304 words.\n",
      "1817-Monroe.txt has a total of 3693 words.\n",
      "1821-Monroe.txt has a total of 4909 words.\n",
      "1825-Adams.txt has a total of 3150 words.\n",
      "1829-Jackson.txt has a total of 1208 words.\n",
      "1833-Jackson.txt has a total of 1267 words.\n",
      "1837-VanBuren.txt has a total of 4171 words.\n",
      "1841-Harrison.txt has a total of 9165 words.\n",
      "1845-Polk.txt has a total of 5196 words.\n",
      "1849-Taylor.txt has a total of 1182 words.\n",
      "1853-Pierce.txt has a total of 3657 words.\n",
      "1857-Buchanan.txt has a total of 3098 words.\n",
      "1861-Lincoln.txt has a total of 4005 words.\n",
      "1865-Lincoln.txt has a total of 785 words.\n",
      "1869-Grant.txt has a total of 1239 words.\n",
      "1873-Grant.txt has a total of 1478 words.\n",
      "1877-Hayes.txt has a total of 2724 words.\n",
      "1881-Garfield.txt has a total of 3239 words.\n",
      "1885-Cleveland.txt has a total of 1828 words.\n",
      "1889-Harrison.txt has a total of 4750 words.\n",
      "1893-Cleveland.txt has a total of 2153 words.\n",
      "1897-McKinley.txt has a total of 4371 words.\n",
      "1901-McKinley.txt has a total of 2450 words.\n",
      "1905-Roosevelt.txt has a total of 1091 words.\n",
      "1909-Taft.txt has a total of 5846 words.\n",
      "1913-Wilson.txt has a total of 1905 words.\n",
      "1917-Wilson.txt has a total of 1656 words.\n",
      "1921-Harding.txt has a total of 3756 words.\n",
      "1925-Coolidge.txt has a total of 4442 words.\n",
      "1929-Hoover.txt has a total of 3890 words.\n",
      "1933-Roosevelt.txt has a total of 2063 words.\n",
      "1937-Roosevelt.txt has a total of 2019 words.\n",
      "1941-Roosevelt.txt has a total of 1536 words.\n",
      "1945-Roosevelt.txt has a total of 637 words.\n",
      "1949-Truman.txt has a total of 2528 words.\n",
      "1953-Eisenhower.txt has a total of 2775 words.\n",
      "1957-Eisenhower.txt has a total of 1917 words.\n",
      "1961-Kennedy.txt has a total of 1546 words.\n",
      "1965-Johnson.txt has a total of 1715 words.\n",
      "1969-Nixon.txt has a total of 2425 words.\n",
      "1973-Nixon.txt has a total of 2028 words.\n",
      "1977-Carter.txt has a total of 1380 words.\n",
      "1981-Reagan.txt has a total of 2801 words.\n",
      "1985-Reagan.txt has a total of 2946 words.\n",
      "1989-Bush.txt has a total of 2713 words.\n",
      "1993-Clinton.txt has a total of 1855 words.\n",
      "1997-Clinton.txt has a total of 2462 words.\n",
      "2001-Bush.txt has a total of 1825 words.\n",
      "2005-Bush.txt has a total of 2376 words.\n",
      "2009-Obama.txt has a total of 2726 words.\n",
      "2013-Obama.txt has a total of 2369 words.\n",
      "2017-Trump.txt has a total of 1693 words.\n"
     ]
    }
   ],
   "source": [
    "for speech in nltk.corpus.inaugural.fileids():\n",
    "    total_words = len(nltk.corpus.inaugural.words(speech))\n",
    "    print(speech + \" has a total of \" + str(total_words) + \" words.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/princessiria/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    " nltk.download('punkt') # helps divides a text into a list of sentences\n",
    "\n",
    "# Lets grab the year, name of president and average word per speech into a dataframe\n",
    "extract = pd.DataFrame([speech[5:].rstrip('.txt'), int(speech[:4]), \n",
    "                        int(len(nltk.corpus.inaugural.words(speech))/len(nltk.corpus.inaugural.sents(speech)))] for speech in nltk.corpus.inaugural.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Washington</td>\n",
       "      <td>1789</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Washington</td>\n",
       "      <td>1793</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adams</td>\n",
       "      <td>1797</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jefferson</td>\n",
       "      <td>1801</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jefferson</td>\n",
       "      <td>1805</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0     1   2\n",
       "0  Washington  1789  64\n",
       "1  Washington  1793  36\n",
       "2       Adams  1797  69\n",
       "3   Jefferson  1801  46\n",
       "4   Jefferson  1805  52"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract.columns = [\"President\", \"Year\", \"AVG_Words_Per_Speech\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>President</th>\n",
       "      <th>Year</th>\n",
       "      <th>AVG_Words_Per_Speech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Washington</td>\n",
       "      <td>1789</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Washington</td>\n",
       "      <td>1793</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adams</td>\n",
       "      <td>1797</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jefferson</td>\n",
       "      <td>1801</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jefferson</td>\n",
       "      <td>1805</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    President  Year  AVG_Words_Per_Speech\n",
       "0  Washington  1789                    64\n",
       "1  Washington  1793                    36\n",
       "2       Adams  1797                    69\n",
       "3   Jefferson  1801                    46\n",
       "4   Jefferson  1805                    52"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vizualize the data.\n",
    "import plotly.express as px\n",
    "from jupyter_dash import JupyterDash\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "from dash.dependencies import Input, Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build App\n",
    "app = JupyterDash(__name__)\n",
    "app.layout = html.Div([\n",
    "    html.H1(\"Inuagural Speech View\"),\n",
    "    dcc.Graph(id='graph'),\n",
    "    html.Label([\n",
    "        \"colorscale\",\n",
    "        dcc.Dropdown(\n",
    "            id='colorscale-dropdown', clearable=False,\n",
    "            value='plasma', options=[\n",
    "                {'label': c, 'value': c}\n",
    "                for c in px.colors.named_colorscales()\n",
    "            ])\n",
    "    ]),\n",
    "])\n",
    "# Define callback to update graph\n",
    "@app.callback(\n",
    "    Output('graph', 'figure'),\n",
    "    [Input(\"colorscale-dropdown\", \"value\")]\n",
    ")\n",
    "def update_figure(colorscale):\n",
    "    return px.scatter(\n",
    "        extract, x=\"Year\", y=\"AVG_Words_Per_Speech\", color=\"Year\",\n",
    "        color_continuous_scale=colorscale,\n",
    "        render_mode=\"webgl\", title=\"AVG_Words_Per_Speech\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:8050/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f9b6b13cfa0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run app and display result inline in the notebook\n",
    "app.run_server(mode='inline')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequency Distribution"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Frequency Distribution: \n",
    "    This refers to the number of times a word token in a text. \n",
    "    That way we are able to get the most common and least common words in a text.\n",
    "    Also take a look at conditional freq distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab words in the shakespeare-hamlet\n",
    "sh_ham = nltk.corpus.gutenberg.words(\"shakespeare-hamlet.txt\")\n",
    "\n",
    "sh_ham_freqDist = nltk.FreqDist(sh_ham)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({',': 2892, '.': 1886, 'the': 860, \"'\": 729, 'and': 606, 'of': 576, 'to': 576, ':': 565, 'I': 553, 'you': 479, ...})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sh_ham_freqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sh_ham_freqDist[\"Hamlet\"] #It can appear to work as counting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sh_ham_20 = sh_ham_freqDist.most_common(20) #Get the 20 most connon words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sh_ham_20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Grams"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This refers to the combination of n words as thet appear in a phrase/sentence. Where n represents number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"I love Special Agent Gibbs, Abby, Dinozo, McGee and NCIS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token wwords from our text\n",
    "tokens = nltk.word_tokenize(sample_text)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Note: nltk has bigrams and trigrams\n",
    "so we have something like nltk.bigrams(tokens) or nltk.trigrams(tokens)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams_words = nltk.bigrams(tokens)\n",
    "for bigramWord in bigrams_words:\n",
    "    print(bigramWord) "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "However, there is one way to handle all of that and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('I', 'love')\n",
      "('love', 'Special')\n",
      "('Special', 'Agent')\n",
      "('Agent', 'Gibbs')\n",
      "('Gibbs', ',')\n",
      "(',', 'Abby')\n",
      "('Abby', ',')\n",
      "(',', 'Dinozo')\n",
      "('Dinozo', ',')\n",
      "(',', 'McGee')\n",
      "('McGee', 'and')\n",
      "('and', 'NCIS')\n"
     ]
    }
   ],
   "source": [
    "bi_ngrams = ngrams(tokens, 2)\n",
    "\n",
    "for bi_ngram in bi_ngrams:\n",
    "    print(bi_ngram)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Probably better we using a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_grams(text, num):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    num_grams = ngrams(tokens, num)\n",
    "    return num_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_grams = n_grams(sample_text, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('I', 'love')\n",
      "('love', 'Special')\n",
      "('Special', 'Agent')\n",
      "('Agent', 'Gibbs')\n",
      "('Gibbs', ',')\n",
      "(',', 'Abby')\n",
      "('Abby', ',')\n",
      "(',', 'Dinozo')\n",
      "('Dinozo', ',')\n",
      "(',', 'McGee')\n",
      "('McGee', 'and')\n",
      "('and', 'NCIS')\n"
     ]
    }
   ],
   "source": [
    "for x in two_grams:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thank you and look forward to my study notes for next week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
